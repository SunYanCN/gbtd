{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering: images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "- [1. Preliminary](#1-preliminary)\n",
    "    - [1.1 Imports](#1-1-import)\n",
    "    - [1.2 Initial data preparations](#1-2-init)\n",
    "- [2. Original data clustering](#2-orig)\n",
    "    - [2.1 Initialization](#2-1-init)\n",
    "    - [2.2 Clustering](#2-2-clust)\n",
    "    - [2.3 Results](#2-3-results)\n",
    "- [3. Common orthogonal basis extraction (COBE)](#3-cobe)\n",
    "    - [3.1 Initialization](#3-1-init)\n",
    "    - [3.2 Computations](#3-2-comp)\n",
    "    - [3.3 Clustering](#3-3-clust)\n",
    "    - [3.4 Results](#3-4-results)\n",
    "- [4. Group independent component analysis (GICA)](#4-gica)\n",
    "    - [4.1 Initialization](#4-1-init)\n",
    "    - [4.2 Computations](#4-2-comp)\n",
    "    - [4.3 Clustering](#4-3-clust)\n",
    "    - [4.4 Results](#4-4-results)\n",
    "- [5. Group (Lr, 1) decomposition (GLRO)](#5-glro)\n",
    "    - [5.1 Initialization](#5-1-init)\n",
    "    - [5.2 Computations](#5-2-comp)\n",
    "    - [5.3 Clustering](#5-3-clust)\n",
    "    - [5.4 Results](#5-4-results)\n",
    "- [6. Group Tucker-(Lr, 1) decomposition (GTLD)](#6-gtld)\n",
    "    - [6.1 Initialization](#6-1-init)\n",
    "    - [6.2 Computations](#6-2-comp)\n",
    "    - [6.3 Clustering](#6-3-clust)\n",
    "    - [6.4 Results](#6-4-results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preliminary <a class=\"anchor\" id=\"1-preliminary\"></a>\n",
    "    \n",
    "- [1.1 Imports](#1-1-import)\n",
    "- [1.2 Initial data preparations](#1-2-init)\n",
    "\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Imports <a class=\"anchor\" id=\"1-1-import\"></a>\n",
    "\n",
    "[Up to section](#1-preliminary) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_NUM_THREADS = 2\n",
    "%env OMP_NUM_THREADS=_NUM_THREADS\n",
    "\n",
    "import numpy as np\n",
    "# import ctypes\n",
    "# mkl_rt = ctypes.CDLL('libmkl_rt.so')\n",
    "# mkl_get_max_threads = mkl_rt.mkl_get_max_threads\n",
    "# def mkl_set_num_threads(cores):\n",
    "#     mkl_rt.mkl_set_num_threads(ctypes.byref(ctypes.c_int(cores)))\n",
    "# mkl_set_num_threads(_NUM_THREADS)\n",
    "# print mkl_get_max_threads() # says 4\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import os\n",
    "import copy\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from src.computational_utilities import reshape\n",
    "\n",
    "from src.cluster.cobe_contrasting import COBEContrast\n",
    "from src.cluster.gica_contrasting import GICAContrast\n",
    "from src.cluster.gtcd_contrasting import GLROContrast\n",
    "from src.cluster.gtcd_contrasting import GTLDContrast\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# clustering performance measures\n",
    "from sklearn.metrics import fowlkes_mallows_score\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "from sklearn.metrics import adjusted_mutual_info_score\n",
    "\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Initial data preparations <a class=\"anchor\" id=\"1-2-init\"></a>\n",
    "\n",
    "[Up to section](#1-preliminary) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dirname = '../data/'\n",
    "data_filename = '../data/eth80_128.npz'\n",
    "\n",
    "df = np.load(data_dirname+data_filename)\n",
    "\n",
    "data, labels = df['data'], df['classes']\n",
    "labels = labels.tolist()\n",
    "Nclasses, Nobjects, Nframes, Npix1, Npix2, Ncolors = data.shape\n",
    "Npix = Npix1*Npix2\n",
    "Nsamples = Nclasses*Nobjects\n",
    "labels = labels*Nobjects\n",
    "\n",
    "new_shape = [-1, Nframes, Npix, Ncolors]\n",
    "data = reshape(data, new_shape)\n",
    "\n",
    "classEncoder = LabelEncoder()\n",
    "classEncoder.fit(labels)\n",
    "y = classEncoder.transform(labels)\n",
    "#classEncoder.inverse_transform(y)\n",
    "\n",
    "random_state = 235\n",
    "n_splits = 4\n",
    "n_repeats = 1\n",
    "separator = RepeatedStratifiedKFold(\n",
    "    n_splits=n_splits, n_repeats=n_repeats, random_state=random_state\n",
    ")\n",
    "\n",
    "dummyX = np.empty(Nsamples)\n",
    "train_indices, test_indices = [], []\n",
    "for train_index, test_index in separator.split(dummyX, y):\n",
    "    train_indices.append(train_index)\n",
    "    test_indices.append(test_index)\n",
    "\n",
    "n_clusters = len(np.unique(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Original data clustering <a class=\"anchor\" id=\"2-orig\"></a>\n",
    "\n",
    "- [2.1 Initialization](#2-1-init)\n",
    "- [2.2 Clustering](#2-2-clust)\n",
    "- [2.3 Results](#2-3-results)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Initialization <a class=\"anchor\" id=\"2-1-init\"></a>\n",
    "\n",
    "[Up to section](#2-orig) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition = [0, 2, 1, 3]\n",
    "X = np.transpose(data, transposition)\n",
    "shapePlain = [Nsamples, Npix*Nframes*Ncolors]\n",
    "X = reshape(X, shapePlain)\n",
    "indices = copy.deepcopy(train_indices)\n",
    "\n",
    "affinity_names = [\n",
    "    'l1', 'l2', 'cosine', 'canberra', 'correlation', 'rbf'\n",
    "]\n",
    "linkage = [\n",
    "    'complete', 'average'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Clustering <a class=\"anchor\" id=\"2-2-clust\"></a>\n",
    "\n",
    "[Up to section](#2-orig) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "save_results_dirname = '../results/clustering/'\n",
    "save_results_filename = 'plain_ETH80_ac'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_results_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_results_dirname)\n",
    "\n",
    "result = np.zeros([n_splits, len(affinity_names), len(linkage), 3])\n",
    "\n",
    "clustAlg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "for k_ind in xrange(n_splits):\n",
    "    print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "    current_index = indices[k_ind]\n",
    "    T = X[current_index].copy()\n",
    "    for k_affinity in xrange(len(affinity_names)):\n",
    "        current_affinity = affinity_names[k_affinity]\n",
    "        for k_linkage in xrange(len(linkage)):\n",
    "            current_linkage = linkage[k_linkage]\n",
    "            clustAlg.linkage = current_linkage\n",
    "            if ((current_affinity == 'canberra') or \n",
    "                (current_affinity == 'correlation') or\n",
    "                (current_affinity == 'rbf')):\n",
    "                clustAlg.affinity = 'precomputed'\n",
    "                if current_affinity == 'rbf':\n",
    "                    D = pairwise_distances(T, metric='euclidean')\n",
    "                    D = - np.exp(-D)\n",
    "                else:\n",
    "                    D = pairwise_distances(T, metric=current_affinity)\n",
    "                pred = clustAlg.fit_predict(D)\n",
    "            else:\n",
    "                clustAlg.affinity = current_affinity\n",
    "                pred = clustAlg.fit_predict(T)\n",
    "            result[k_ind, k_affinity, k_linkage, 0] = adjusted_rand_score(\n",
    "                y[indices[k_ind]],\n",
    "                pred\n",
    "            )\n",
    "            result[k_ind, k_affinity, k_linkage, 1] = adjusted_mutual_info_score(\n",
    "                y[indices[k_ind]],\n",
    "                pred\n",
    "            )\n",
    "            result[k_ind, k_affinity, k_linkage, 2] = fowlkes_mallows_score(\n",
    "                y[indices[k_ind]],\n",
    "                pred\n",
    "            )\n",
    "            np.savez_compressed(\n",
    "                save_results_dirname+save_results_filename, result=result, affinity_names=affinity_names,\n",
    "                indices=indices, linkage=linkage\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Results <a class=\"anchor\" id=\"2-3-results\"></a>\n",
    "\n",
    "[Up to section](#2-orig) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.544705000668 (array([4]), array([1]))\n",
      "average correlation\n",
      " ARI  AMI  FMS\n",
      "0.545 0.669 0.615\n",
      "complete canberra\n",
      "0.423 0.556 0.509\n"
     ]
    }
   ],
   "source": [
    "results_dirname = '../results/clustering/'\n",
    "results_filename = 'plain_ETH80_ac.npz'\n",
    "\n",
    "df = np.load(results_dirname + results_filename)\n",
    "\n",
    "linkageList = df['linkage']\n",
    "affinityList = df['affinity_names']\n",
    "\n",
    "a = np.mean(df['result'], axis=0)[:, :]\n",
    "maxval = a[:, :, 0].max()\n",
    "maxind = np.where(a[:, :, 0] == maxval)\n",
    "print maxval, maxind\n",
    "print linkageList[maxind[1][0]], affinityList[maxind[0][0]]\n",
    "v1, v2, v3 = a[maxind[0][0], maxind[1][0]]\n",
    "print \" ARI  AMI  FMS\"\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)\n",
    "print linkageList[0], affinityList[3]\n",
    "v1, v2, v3 = a[3, 0]\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common orthogonal basis extraction (COBE) <a class=\"anchor\" id=\"3-cobe\"></a>\n",
    "\n",
    "- [3.1 Initialization](#3-1-init)\n",
    "- [3.2 Computations](#3-2-comp)\n",
    "- [3.3 Clustering](#3-3-clust)\n",
    "- [3.4 Results](#3-4-results)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Initialization <a class=\"anchor\" id=\"3-1-init\"></a>\n",
    "\n",
    "[Up to section](#3-cobe) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition = [0, 2, 1, 3]\n",
    "X = np.transpose(data, transposition)\n",
    "shapeCOBE = [Nsamples, Npix, Nframes*Ncolors]\n",
    "X = reshape(X, shapeCOBE)\n",
    "indices = copy.deepcopy(train_indices)\n",
    "commonRanks = 1+np.arange(10)   \n",
    "\n",
    "affinity_names = [\n",
    "    'l1', 'l2', 'cosine', 'canberra', 'correlation', 'rbf'\n",
    "]\n",
    "linkage = [\n",
    "    'complete', 'average'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Computations <a class=\"anchor\" id=\"3-2-comp\"></a>\n",
    "\n",
    "[Up to section](#3-cobe) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dirname = '../models/clustering/cobe/'\n",
    "save_filename_base = 'COBE_contrasted_ETH80'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_model_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_model_dirname)\n",
    "\n",
    "times = np.zeros([len(commonRanks), len(indices)])\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_ind in xrange(n_splits):\n",
    "        print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "        current_index = indices[k_ind]\n",
    "        t1 = time.clock()\n",
    "        contrasting = COBEContrast(\n",
    "            n_clusters, commonRank, shapeObject=shapeCOBE[1:], maxitnum=100, epsilon=1e-5\n",
    "        )\n",
    "        t2 = time.clock()\n",
    "        times[k_comr, k_ind] = t2-t1\n",
    "        T = contrasting.fit_transform(X[current_index], commonRank)\n",
    "        save_filename = str(k_ind) + '_' + save_filename_base + '_crank='+str(commonRank)\n",
    "        np.savez_compressed(save_model_dirname+save_filename, T=T, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Clustering <a class=\"anchor\" id=\"3-3-clust\"></a>\n",
    "\n",
    "[Up to section](#3-cobe) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = '../models/clustering/cobe/'\n",
    "model_filename_base = 'COBE_contrasted_ETH80'\n",
    "\n",
    "save_results_dirname = '../results/clustering/'\n",
    "save_results_filename = 'cobe_ETH80_ac'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_results_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_results_dirname)\n",
    "\n",
    "\n",
    "result = np.zeros([len(commonRanks), n_splits, len(affinity_names), len(linkage), 3])\n",
    "clustAlg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_ind in xrange(n_splits):#len(indices)):\n",
    "        print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "        current_index = indices[k_ind]\n",
    "        current_df = np.load(\n",
    "            model_dirname+str(k_ind)+'_'+model_filename_base+'_crank='+str(commonRank)+\\\n",
    "            '.npz'\n",
    "        )\n",
    "        T = current_df['T']\n",
    "        T = reshape(T, [T.shape[0], -1])\n",
    "        for k_affinity in xrange(len(affinity_names)):\n",
    "            current_affinity = affinity_names[k_affinity]\n",
    "            for k_linkage in xrange(len(linkage)):\n",
    "                current_linkage = linkage[k_linkage]\n",
    "                clustAlg.linkage = current_linkage\n",
    "                if ((current_affinity == 'canberra') or \n",
    "                    (current_affinity == 'correlation') or\n",
    "                    (current_affinity == 'rbf')):\n",
    "                    clustAlg.affinity = 'precomputed'\n",
    "                    if current_affinity == 'rbf':\n",
    "                        D = pairwise_distances(T, metric='euclidean')\n",
    "                        D = - np.exp(-D)\n",
    "                    else:\n",
    "                        D = pairwise_distances(T, metric=current_affinity)\n",
    "                    pred = clustAlg.fit_predict(D)\n",
    "                else:\n",
    "                    clustAlg.affinity = current_affinity\n",
    "                    pred = clustAlg.fit_predict(T)\n",
    "                result[k_comr, k_ind, k_affinity, k_linkage, 0] = adjusted_rand_score(\n",
    "                    y[indices[k_ind]],\n",
    "                    pred\n",
    "                )\n",
    "                result[k_comr, k_ind, k_affinity, k_linkage, 1] = adjusted_mutual_info_score(\n",
    "                    y[indices[k_ind]],\n",
    "                    pred\n",
    "                )\n",
    "                result[k_comr, k_ind, k_affinity, k_linkage, 2] = fowlkes_mallows_score(\n",
    "                    y[indices[k_ind]],\n",
    "                    pred\n",
    "                )\n",
    "                np.savez_compressed(\n",
    "                    save_results_dirname+save_results_filename, result=result,\n",
    "                    affinity_names=affinity_names, indices=indices,\n",
    "                    linkage=linkage\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Results <a class=\"anchor\" id=\"3-4-results\"></a>\n",
    "\n",
    "[Up to section](#3-cobe) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.692952179571 (array([8]), array([3]), array([0]))\n",
      "complete canberra 9\n",
      " ARI  AMI  FMS\n",
      "0.693 0.775 0.732\n"
     ]
    }
   ],
   "source": [
    "results_dirname = '../results/clustering/'\n",
    "results_filename = 'cobe_ETH80_ac.npz'\n",
    "\n",
    "df = np.load(results_dirname+results_filename)\n",
    "\n",
    "linkageList = df['linkage']\n",
    "affinityList = df['affinity_names']\n",
    "commonRanks = 1+np.arange(a.shape[0])\n",
    "\n",
    "a = np.mean(df['result'], axis=1)\n",
    "maxval = a[:, :, :, 0].max()\n",
    "maxind = np.where(a[:, :, :, 0] == maxval)\n",
    "print maxval, maxind\n",
    "print linkageList[maxind[2][0]], affinityList[maxind[1][0]], commonRanks[maxind[0][0]]\n",
    "v1, v2, v3 = a[maxind[0][0], maxind[1][0], maxind[2][0]]\n",
    "print \" ARI  AMI  FMS\"\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Group independent component analysis (GICA) <a class=\"anchor\" id=\"4-gica\"></a>\n",
    "\n",
    "- [4.1 Initialization](#4-1-init)\n",
    "- [4.2 Computations](#4-2-comp)\n",
    "- [4.3 Clustering](#4-3-clust)\n",
    "- [4.4 Results](#4-4-results)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Initialization <a class=\"anchor\" id=\"4-1-init\"></a>\n",
    "\n",
    "[Up to section](#4-gica) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition = [0, 2, 1, 3]\n",
    "X = np.transpose(data, transposition)\n",
    "shapeGICA = [Nsamples, Npix, Nframes*Ncolors]\n",
    "X = reshape(X, shapeGICA)\n",
    "indices = copy.deepcopy(train_indices)\n",
    "commonRanks = 1+np.arange(10)\n",
    "individualRanks = 1+np.arange(10)\n",
    "\n",
    "affinity_names = [\n",
    "    'l1', 'l2', 'cosine', 'canberra', 'correlation', 'rbf'\n",
    "]\n",
    "linkage = [\n",
    "    'complete', 'average'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Computations <a class=\"anchor\" id=\"4-2-comp\"></a>\n",
    "\n",
    "[Up to section](#4-gica) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dirname = '../models/clustering/gica/'\n",
    "save_filename_base = 'GICA_contrasted_ETH80'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_model_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_model_dirname)\n",
    "\n",
    "times = np.zeros([len(commonRanks), len(individualRanks), len(indices)])\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            t1 = time.clock()\n",
    "            contrasting = GICAContrast(\n",
    "                n_clusters, individualRank, commonRank, shapeObject=shapeGICA[1:],\n",
    "                maxitnum=100, epsilon=1e-5\n",
    "            )\n",
    "            t2 = time.clock()\n",
    "            times[k_comr, k_ind] = t2-t1\n",
    "            T = contrasting.fit_transform(X[current_index], individualRank, commonRank)\n",
    "            save_filename = str(k_ind) + '_' + save_filename_base + '_crank='+str(commonRank)\n",
    "            save_filename += '_irank='+str(individualRank)\n",
    "            np.savez_compressed(save_model_dirname+save_filename, T=T, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Clustering <a class=\"anchor\" id=\"4-3-clust\"></a>\n",
    "\n",
    "[Up to section](#4-gica) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = '../models/clustering/gica/'\n",
    "model_filename_base = 'GICA_contrasted_ETH80'\n",
    "\n",
    "save_results_dirname = '../results/clustering/'\n",
    "save_results_filename = 'gica_ETH80_ac'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_results_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_results_dirname)\n",
    "\n",
    "\n",
    "result = np.zeros([len(commonRanks), len(individualRanks), n_splits, len(affinity_names), len(linkage), 3])\n",
    "clustAlg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):#len(indices)):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            current_df = np.load(\n",
    "                model_dirname+str(k_ind)+'_'+model_filename_base+'_crank='+str(commonRank)+\\\n",
    "                '_irank='+str(individualRank)+'.npz'\n",
    "            )\n",
    "            T = current_df['T']\n",
    "            T = reshape(T, [T.shape[0], -1])\n",
    "            for k_affinity in xrange(len(affinity_names)):\n",
    "                current_affinity = affinity_names[k_affinity]\n",
    "                for k_linkage in xrange(len(linkage)):\n",
    "                    current_linkage = linkage[k_linkage]\n",
    "                    clustAlg.linkage = current_linkage\n",
    "                    if ((current_affinity == 'canberra') or \n",
    "                        (current_affinity == 'correlation') or\n",
    "                        (current_affinity == 'rbf')):\n",
    "                        clustAlg.affinity = 'precomputed'\n",
    "                        if current_affinity == 'rbf':\n",
    "                            D = pairwise_distances(T, metric='euclidean')\n",
    "                            D = - np.exp(-D)\n",
    "                        else:\n",
    "                            D = pairwise_distances(T, metric=current_affinity)\n",
    "                        pred = clustAlg.fit_predict(D)\n",
    "                    else:\n",
    "                        clustAlg.affinity = current_affinity\n",
    "                        pred = clustAlg.fit_predict(T)\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 0] = adjusted_rand_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 1] = adjusted_mutual_info_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 2] = fowlkes_mallows_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    np.savez_compressed(\n",
    "                        save_results_dirname+save_results_filename, result=result,\n",
    "                        affinity_names=affinity_names, indices=indices, linkage=linkage\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Results <a class=\"anchor\" id=\"4-4-results\"></a>\n",
    "\n",
    "[Up to section](#4-gica) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.614945400081 (array([9, 9, 9, 9, 9, 9, 9, 9, 9, 9]), array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), array([3, 3, 3, 3, 3, 3, 3, 3, 3, 3]), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))\n",
      "('complete', 'canberra', 10, 1)\n",
      " ARI  AMI  FMS\n",
      "0.615 0.728 0.669\n"
     ]
    }
   ],
   "source": [
    "results_dirname = '../results/clustering/'\n",
    "results_filename = 'gica_ETH80_ac.npz'\n",
    "\n",
    "df = np.load(results_dirname+results_filename)\n",
    "linkageList = df['linkage']\n",
    "affinityList = df['affinity_names']\n",
    "commonRanks = 1+np.arange(a.shape[0])\n",
    "individualRanks = 1+np.arange(a.shape[1])\n",
    "\n",
    "a = np.mean(df['result'], axis=2)\n",
    "maxval = a[:, :, :, :, 0].max()\n",
    "maxind = np.where(a[:, :, :, :, 0] == maxval)\n",
    "print maxval, maxind\n",
    "\n",
    "print (\n",
    "    linkageList[maxind[3][0]], affinityList[maxind[2][0]],\n",
    "    commonRanks[maxind[0][0]], individualRanks[maxind[1][0]]\n",
    ")\n",
    "\n",
    "\n",
    "v1, v2, v3 = a[maxind[0][0], maxind[1][0], maxind[2][0], maxind[3][0]]\n",
    "print \" ARI  AMI  FMS\"\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Group (Lr, 1) decomposition (GLRO) <a class=\"anchor\" id=\"5-glro\"></a>\n",
    "\n",
    "- [5.1 Initialization](#5-1-init)\n",
    "- [5.2 Computations](#5-2-comp)\n",
    "- [5.3 Clustering](#5-3-clust)\n",
    "- [5.4 Results](#5-4-results)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Initialization <a class=\"anchor\" id=\"5-1-init\"></a>\n",
    "\n",
    "[Up to section](#5-glro) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition = [0, 2, 1, 3]\n",
    "X = np.transpose(data, transposition)\n",
    "shapeGLRO = [Nsamples, Npix, Nframes, Ncolors]\n",
    "X = reshape(X, shapeGLRO)\n",
    "indices = copy.deepcopy(train_indices)\n",
    "commonRanks = 1+np.arange(10)\n",
    "individualRanks = 1+np.arange(10)\n",
    "sourceModes = [0]\n",
    "\n",
    "affinity_names = [\n",
    "    'l1', 'l2', 'cosine', 'canberra', 'correlation', 'rbf'\n",
    "]\n",
    "linkage = [\n",
    "    'complete', 'average'\n",
    "]\n",
    "\n",
    "method_name = 'als'\n",
    "constraint_method = 'projected'\n",
    "maxitnum = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Computations <a class=\"anchor\" id=\"5-2-comp\"></a>\n",
    "\n",
    "[Up to section](#5-glro) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dirname = '../models/clustering/glro/'\n",
    "save_filename_base = 'GLRO_contrasted_ETH80'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_model_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_model_dirname)\n",
    "\n",
    "times = np.zeros([len(commonRanks), len(individualRanks), len(indices)])\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            \n",
    "            contrasting = GLROContrast(\n",
    "                n_clusters, individualRank, commonRank, shapeObject=shapeGLRO[1:],\n",
    "                maxitnum=maxitnum, epsilon=1e-5, sourceModes=sourceModes,\n",
    "                method=method_name, nShortModes=None, constraintMethod=constraint_method,\n",
    "                fullModesConstraint=None\n",
    "            )\n",
    "            \n",
    "            t1 = time.clock()\n",
    "            T = contrasting.fit_transform(\n",
    "                X[current_index], individualRank, commonRank, \n",
    "                 verbose=True, recover=True\n",
    "            )\n",
    "            t2 = time.clock()\n",
    "            times[k_comr, k_ind] = t2-t1\n",
    "            save_filename = str(k_ind) + '_' + save_filename_base + '_crank='+str(commonRank)\n",
    "            save_filename += '_irank='+str(individualRank)\n",
    "            np.savez_compressed(save_model_dirname+save_filename, T=T, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Clustering <a class=\"anchor\" id=\"5-3-clust\"></a>\n",
    "\n",
    "[Up to section](#5-glro) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = '../models/clustering/glro/'\n",
    "model_filename_base = 'GLRO_contrasted_ETH80'\n",
    "\n",
    "save_results_dirname = '../results/clustering/'\n",
    "save_results_filename = 'glro_ETH80_ac'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_results_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_results_dirname)\n",
    "\n",
    "result = np.zeros([len(commonRanks), len(individualRanks), n_splits, len(affinity_names), len(linkage), 3])\n",
    "clustAlg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):#len(indices)):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            current_df = np.load(\n",
    "                model_dirname+str(k_ind)+'_'+model_filename_base+'_crank='+str(commonRank)+\\\n",
    "                '_irank='+str(individualRank)+'.npz'\n",
    "            )\n",
    "            T = current_df['T']\n",
    "            T = np.transpose(T, [3, 0, 1, 2])\n",
    "            T = reshape(T, [T.shape[0], -1])\n",
    "            for k_affinity in xrange(len(affinity_names)):\n",
    "                current_affinity = affinity_names[k_affinity]\n",
    "                for k_linkage in xrange(len(linkage)):\n",
    "                    current_linkage = linkage[k_linkage]\n",
    "                    clustAlg.linkage = current_linkage\n",
    "                    if ((current_affinity == 'canberra') or \n",
    "                        (current_affinity == 'correlation') or\n",
    "                        (current_affinity == 'rbf')):\n",
    "                        clustAlg.affinity = 'precomputed'\n",
    "                        if current_affinity == 'rbf':\n",
    "                            D = pairwise_distances(T, metric='euclidean')\n",
    "                            D = - np.exp(-D)\n",
    "                        else:\n",
    "                            D = pairwise_distances(T, metric=current_affinity)\n",
    "                        pred = clustAlg.fit_predict(D)\n",
    "                    else:\n",
    "                        clustAlg.affinity = current_affinity\n",
    "                        pred = clustAlg.fit_predict(T)\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 0] = adjusted_rand_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 1] = adjusted_mutual_info_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 2] = fowlkes_mallows_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    np.savez_compressed(\n",
    "                        save_results_dirname+save_results_filename, result=result,\n",
    "                        affinity_names=affinity_names, indices=indices,\n",
    "                        linkage=linkage\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Results <a class=\"anchor\" id=\"5-4-results\"></a>\n",
    "\n",
    "[Up to section](#5-glro) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.565152558846 (array([7]), array([2]), array([3]), array([0]))\n",
      "('complete', 'canberra', 8, 3)\n",
      " ARI  AMI  FMS\n",
      "0.565 0.669 0.629\n"
     ]
    }
   ],
   "source": [
    "results_dirname = '../results/clustering/'\n",
    "results_filename = 'glro_ETH80_ac.npz'\n",
    "\n",
    "df = np.load(results_dirname+results_filename)\n",
    "linkageList = df['linkage']\n",
    "affinityList = df['affinity_names']\n",
    "commonRanks = 1+np.arange(a.shape[0])\n",
    "individualRanks = 1+np.arange(a.shape[1])\n",
    "\n",
    "a = np.mean(df['result'], axis=2)\n",
    "maxval = a[:, :, :, :, 0].max()\n",
    "maxind = np.where(a[:, :, :, :, 0] == maxval)\n",
    "print maxval, maxind\n",
    "print (\n",
    "    linkageList[maxind[3][0]], affinityList[maxind[2][0]], commonRanks[maxind[0][0]],\n",
    "    individualRanks[maxind[1][0]]\n",
    ")\n",
    "v1, v2, v3 = a[maxind[0][0], maxind[1][0], maxind[2][0], maxind[3][0]]\n",
    "print \" ARI  AMI  FMS\"\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Group Tucker-(Lr, 1) decomposition (GTLD) <a class=\"anchor\" id=\"6-gtld\"></a>\n",
    "\n",
    "- [6.1 Initialization](#6-1-init)\n",
    "- [6.2 Computations](#6-2-comp)\n",
    "- [6.3 Clustering](#6-3-clust)\n",
    "- [6.4 Results](#6-4-results)\n",
    "\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Initialization <a class=\"anchor\" id=\"6-1-init\"></a>\n",
    "\n",
    "[Up to section](#6-gtld) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposition = [0, 2, 1, 3]\n",
    "X = np.transpose(data, transposition)\n",
    "shapeGTLD = [Nsamples, Npix, Nframes, Ncolors]\n",
    "X = reshape(X, shapeGTLD)\n",
    "indices = copy.deepcopy(train_indices)\n",
    "commonRanks = 1+np.arange(10)\n",
    "individualRanks = 1+np.arange(10)\n",
    "sourceModes = [0]\n",
    "\n",
    "affinity_names = [\n",
    "    'l1', 'l2', 'cosine', 'canberra', 'correlation', 'rbf'\n",
    "]\n",
    "linkage = [\n",
    "    'complete', 'average'\n",
    "]\n",
    "\n",
    "method_name = 'als'\n",
    "constraint_method = 'projected'\n",
    "maxitnum = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Computations <a class=\"anchor\" id=\"6-2-comp\"></a>\n",
    "\n",
    "[Up to section](#6-gtld) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_dirname = '../models/clustering/gtld/'\n",
    "save_filename_base = 'GTLD_contrasted_ETH80'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_model_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_model_dirname)\n",
    "\n",
    "\n",
    "times = np.zeros([len(commonRanks), len(individualRanks), len(indices)])\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):#len(indices)):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            contrasting = GTLDContrast(\n",
    "                n_clusters, individualRank, commonRank, shapeObject=shapeGTLD[1:],\n",
    "                maxitnum=maxitnum, epsilon=1e-5, sourceModes=sourceModes,\n",
    "                method=method_name, nShortModes=None, constraintMethod=constraint_method,\n",
    "                fullModesConstraint=None, modeSizeFirstPriority=True\n",
    "            )\n",
    "            \n",
    "            t1 = time.clock()\n",
    "            T = contrasting.fit_transform(\n",
    "                X[current_index], individualRank, commonRank, \n",
    "                verbose=True, recover=True\n",
    "            )\n",
    "            t2 = time.clock()\n",
    "            times[k_comr, k_ind] = t2-t1\n",
    "            save_filename = str(k_ind) + '_' + save_filename_base + '_crank='+str(commonRank)\n",
    "            save_filename += '_irank='+str(individualRank)\n",
    "            np.savez_compressed(save_model_dirname+save_filename, T=T, times=times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Clustering <a class=\"anchor\" id=\"6-3-clust\"></a>\n",
    "\n",
    "[Up to section](#6-gtld) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dirname = '../models/clustering/gtld/'\n",
    "model_filename_base = 'GTLD_contrasted_ETH80'\n",
    "\n",
    "save_results_dirname = '../results/clustering/'\n",
    "save_results_filename = 'gtld_ETH80_ac'\n",
    "\n",
    "try:\n",
    "    os.makedirs(save_results_dirname)\n",
    "except:\n",
    "    print 'Already exist: %s' % (save_results_dirname)\n",
    "\n",
    "result = np.zeros([len(commonRanks), len(individualRanks), n_splits, len(affinity_names), len(linkage), 3])\n",
    "clustAlg = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "\n",
    "for k_comr in xrange(len(commonRanks)):\n",
    "    commonRank = commonRanks[k_comr]\n",
    "    print '\\t %d rank' % (commonRank)\n",
    "    for k_indr in xrange(len(individualRanks)):\n",
    "        individualRank = individualRanks[k_indr]\n",
    "        print '\\t\\t %d rank' % (individualRank)\n",
    "        for k_ind in xrange(n_splits):#len(indices)):\n",
    "            print 'Index %d / %d' % (k_ind+1, len(indices))\n",
    "            current_index = indices[k_ind]\n",
    "            current_df = np.load(\n",
    "                model_dirname+str(k_ind)+'_'+model_filename_base+'_crank='+str(commonRank)+\\\n",
    "                '_irank='+str(individualRank)+'.npz'\n",
    "            )\n",
    "            T = current_df['T']\n",
    "            T = np.transpose(T, [3, 0, 1, 2])\n",
    "            T = reshape(T, [T.shape[0], -1])\n",
    "            for k_affinity in xrange(len(affinity_names)):\n",
    "                current_affinity = affinity_names[k_affinity]\n",
    "                for k_linkage in xrange(len(linkage)):\n",
    "                    current_linkage = linkage[k_linkage]\n",
    "                    clustAlg.linkage = current_linkage\n",
    "                    if ((current_affinity == 'canberra') or \n",
    "                        (current_affinity == 'correlation') or\n",
    "                        (current_affinity == 'rbf')):\n",
    "                        clustAlg.affinity = 'precomputed'\n",
    "                        if current_affinity == 'rbf':\n",
    "                            D = pairwise_distances(T, metric='euclidean')\n",
    "                            D = - np.exp(-D)\n",
    "                        else:\n",
    "                            D = pairwise_distances(T, metric=current_affinity)\n",
    "                        pred = clustAlg.fit_predict(D)\n",
    "                    else:\n",
    "                        clustAlg.affinity = current_affinity\n",
    "                        pred = clustAlg.fit_predict(T)\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 0] = adjusted_rand_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 1] = adjusted_mutual_info_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    result[k_comr, k_indr, k_ind, k_affinity, k_linkage, 2] = fowlkes_mallows_score(\n",
    "                        y[indices[k_ind]],\n",
    "                        pred\n",
    "                    )\n",
    "                    np.savez_compressed(\n",
    "                        save_results_dirname+save_results_filename, result=result,\n",
    "                        affinity_names=affinity_names, indices=indices,\n",
    "                        linkage=linkage\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Results <a class=\"anchor\" id=\"6-4-results\"></a>\n",
    "\n",
    "[Up to section](#6-gtld) $\\quad$\n",
    "[Back to contents](#Contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.582585887064 (array([7]), array([2]), array([3]), array([0]))\n",
      "('complete', 'canberra', 8, 3)\n",
      " ARI  AMI  FMS\n",
      "0.583 0.693 0.640\n"
     ]
    }
   ],
   "source": [
    "results_dirname = '../results/clustering/'\n",
    "results_filename = 'gtld_ETH80_ac.npz'\n",
    "\n",
    "df = np.load(results_dirname+results_filename)\n",
    "linkageList = df['linkage']\n",
    "affinityList = df['affinity_names']\n",
    "commonRanks = 1+np.arange(a.shape[0])\n",
    "individualRanks = 1+np.arange(a.shape[1])\n",
    "\n",
    "a = np.mean(df['result'], axis=2)\n",
    "maxval = a[:, :, :, :, 0].max()\n",
    "maxind = np.where(a[:, :, :, :, 0] == maxval)\n",
    "print maxval, maxind\n",
    "print (\n",
    "    linkageList[maxind[3][0]], affinityList[maxind[2][0]], commonRanks[maxind[0][0]],\n",
    "    individualRanks[maxind[1][0]]\n",
    ")\n",
    "v1, v2, v3 = a[maxind[0][0], maxind[1][0], maxind[2][0], maxind[3][0]]\n",
    "print \" ARI  AMI  FMS\"\n",
    "print \"%.3f %.3f %.3f\" % (v1, v2, v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
